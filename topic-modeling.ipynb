{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common_freq_words(words, n=10):\n",
    "    fdist = FreqDist(words)\n",
    "    for word, frequency in fdist.most_common(n):\n",
    "        print ('{};{}'.format(word, frequency))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_book(name):\n",
    "    file = open(name,'rb')\n",
    "    book = PyPDF2.PdfFileReader(file)\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(book, start, end):\n",
    "    text = ''\n",
    "    pages = []\n",
    "    for idx in range(start,end,1):\n",
    "        page = book.getPage(idx)\n",
    "        text = page.extractText()\n",
    "        # text.encode('utf-8')\n",
    "        pages.append(text)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_list(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    txt = nltk.Text(tokens)\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    return [w.lower() for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    words = nltk.tokenize.word_tokenize(filtered_sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations_and_small_words(words, word_len):\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [w for w in words if len(w)>word_len]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return [wordnet_lemmatizer.lemmatize(w).encode('utf-8') for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    return nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_text(word_tag_pairs, pos):\n",
    "    word_fd = [word for (word, tag) in pos_text if tag in pos]\n",
    "    return set(word_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(doc):\n",
    "    from gensim import corpora\n",
    "    dictionary = corpora.Dictionary(doc)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_matrix(doc_clean, dictionary):\n",
    "    import gensim\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lda_model(doc_term_matrix, dictionary, topic_count):\n",
    "    from gensim.models.ldamodel import LdaModel\n",
    "    ldamodel = LdaModel(doc_term_matrix, num_topics=topic_count, id2word = dictionary, passes=50)\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tfidf_model(corpus):\n",
    "    from gensim.models import TfidfModel\n",
    "    model = TfidfModel(corpus)  # fit model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hdp_model(doc_term_matrix, dictionary):\n",
    "    from gensim.models import HdpModel\n",
    "    hdp = HdpModel(doc_term_matrix, dictionary)\n",
    "    return hdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = 'Influence the psychology of persuasion.pdf'\n",
    "topics = {\n",
    "    'chap1':{'start':10,'end':22},\\\n",
    "    'chap2':{'start':23,'end':52},\\\n",
    "    'chap3':{'start':53,'end':96},\\\n",
    "    'chap4':{'start':97,'end':135},\\\n",
    "    'chap5':{'start':136,'end':166},\\\n",
    "    'chap6':{'start':167,'end':187},\\\n",
    "    'chap7':{'start':188,'end':214}\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.023*appeal + 0.019*becomes + 0.017*cause'), (1, u'0.046*stand + 0.026*term + 0.022*cessful'), (2, u'0.030*importance + 0.020*result + 0.020*temple')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "1\n",
      "chap7 [(0, u'0.024*\"health\" + 0.024*\"breast\" + 0.017*\"potential\"'), (1, u'0.017*\"could\" + 0.017*\"something\" + 0.017*\"thought\"'), (2, u'0.016*\"temple\" + 0.016*\"never\" + 0.016*\"become\"')]\n",
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.026*play + 0.025*action + 0.021*general'), (1, u'0.027*carry + 0.024*blurred + 0.023*ongoing'), (2, u'0.029*importance + 0.028*spoke + 0.023*intent')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap6\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=220)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.031*leave + 0.027*grunt + 0.025*teacher'), (1, u'0.041*lead + 0.030*leave + 0.027*beg'), (2, u'0.025*effect + 0.022*strong + 0.020*confused')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "2\n",
      "chap7 [(0, u'0.017*\"health\" + 0.017*\"breast\" + 0.017*\"temple\"'), (1, u'0.025*\"something\" + 0.025*\"thought\" + 0.025*\"rare\"'), (2, u'0.018*\"could\" + 0.018*\"scarcity\" + 0.018*\"become\"')]\n",
      "chap6 [(0, u'0.033*\"teacher\" + 0.025*\"answer\" + 0.025*\"test\"'), (1, u'0.054*\"shock\" + 0.028*\"teacher\" + 0.015*\"make\"'), (2, u'0.031*\"teacher\" + 0.022*\"delivers\" + 0.022*\"question\"')]\n",
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.028*case + 0.022*keenlyaware + 0.020*attractive'), (1, u'0.029*special + 0.022*influence + 0.021*thought'), (2, u'0.036*availability + 0.032*antique + 0.030*failing')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap6\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=220)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.042*though + 0.036*think + 0.033*come'), (1, u'0.027*another + 0.027*writhe + 0.024*sends'), (2, u'0.032*chance + 0.027*plenty + 0.027*although')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap5\n",
      "18\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 18\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=18, num_nnz=210)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.034*housewife + 0.031*order + 0.030*attraction'), (1, u'0.029*appear + 0.024*side + 0.022*something'), (2, u'0.036*unknown + 0.032*avoid + 0.026*warmth')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "3\n",
      "chap7 [(0, u'0.016*\"ring\" + 0.015*\"scarcity\" + 0.015*\"phone\"'), (1, u'0.019*\"something\" + 0.019*\"never\" + 0.019*\"thought\"'), (2, u'0.016*\"health\" + 0.016*\"breast\" + 0.016*\"potential\"')]\n",
      "chap6 [(0, u'0.050*\"shock\" + 0.026*\"punishment\" + 0.018*\"make\"'), (1, u'0.045*\"teacher\" + 0.038*\"answer\" + 0.038*\"shock\"'), (2, u'0.042*\"teacher\" + 0.018*\"question\" + 0.018*\"test\"')]\n",
      "chap5 [(0, u'0.019*\"party\" + 0.019*\"sale\" + 0.019*\"friendship\"'), (1, u'0.038*\"friend\" + 0.020*\"home\" + 0.020*\"product\"'), (2, u'0.024*\"party\" + 0.024*\"tupperware\" + 0.014*\"buy\"')]\n",
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.045*market + 0.028*intent + 0.019*number'), (1, u'0.036*benefit + 0.031*average + 0.027*principle'), (2, u'0.026*human + 0.026*month + 0.026*valuedrobert')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap6\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=220)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.037*intercom + 0.036*continue + 0.021*give'), (1, u'0.034*accumulate + 0.029*say + 0.028*question\\xf0and'), (2, u'0.047*let + 0.026*get + 0.025*hold')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap5\n",
      "18\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 18\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=18, num_nnz=210)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.027*salesperson + 0.025*perfectly + 0.023*company'), (1, u'0.040*wonder + 0.030*product + 0.030*perfectly'), (2, u'0.039*spoke + 0.024*chatting + 0.023*yes')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap4\n",
      "22\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 22\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=22, num_nnz=223)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.056*view + 0.047*anyone + 0.038*glutted'), (1, u'0.028*defining + 0.025*others + 0.020*indicates'), (2, u'0.048*precisely + 0.027*effective + 0.024*appropriate')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "4\n",
      "chap7 [(0, u'0.022*\"potential\" + 0.022*\"breast\" + 0.022*\"health\"'), (1, u'0.021*\"temple\" + 0.015*\"never\" + 0.015*\"become\"'), (2, u'0.029*\"could\" + 0.021*\"much\" + 0.021*\"money\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). [pyplot.py:537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chap6 [(0, u'0.030*\"shock\" + 0.017*\"level\" + 0.017*\"first\"'), (1, u'0.039*\"teacher\" + 0.034*\"test\" + 0.033*\"shock\"'), (2, u'0.043*\"shock\" + 0.034*\"teacher\" + 0.011*\"error\"')]\n",
      "chap5 [(0, u'0.039*\"friend\" + 0.024*\"like\" + 0.017*\"compliance\"'), (1, u'0.022*\"product\" + 0.022*\"social\" + 0.013*\"hostess\"'), (2, u'0.038*\"party\" + 0.024*\"tupperware\" + 0.024*\"sale\"')]\n",
      "chap4 [(0, u'0.028*\"executive\" + 0.019*\"know\" + 0.019*\"television\"'), (1, u'0.028*\"correct\" + 0.028*\"behavior\" + 0.019*\"material\"'), (2, u'0.022*\"action\" + 0.022*\"laughter\" + 0.015*\"principle\"')]\n",
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.029*general + 0.029*cessful + 0.022*opportunitiesseem'), (1, u'0.036*unknown + 0.029*became + 0.026*compelling'), (2, u'0.037*stance + 0.024*phenomenon + 0.021*stirring')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap6\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=220)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.035*announces + 0.026*teacher + 0.023*\\xaano'), (1, u'0.031*disruptive + 0.027*theteacher + 0.024*deliver'), (2, u'0.031*progress + 0.029*failure + 0.029*volt')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap5\n",
      "18\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 18\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=18, num_nnz=210)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.041*growing + 0.038*stranger + 0.034*twice'), (1, u'0.034*seem + 0.031*found + 0.031*gotten'), (2, u'0.027*arranges + 0.024*compelling + 0.022*woman')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap4\n",
      "22\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 22\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=22, num_nnz=223)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.044*real + 0.043*indicates + 0.032*comic'), (1, u'0.030*full + 0.027*flotsam + 0.027*behavior'), (2, u'0.034*quality + 0.028*decide + 0.022*dinner')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap3\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=236)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.030*told + 0.026*wedding + 0.023*newcomb'), (1, u'0.031*became + 0.023*idea + 0.022*heart'), (2, u'0.028*really + 0.019*issue + 0.019*drink')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "5\n",
      "chap7 [(0, u'0.022*\"potential\" + 0.022*\"health\" + 0.022*\"breast\"'), (1, u'0.018*\"something\" + 0.018*\"thought\" + 0.018*\"rare\"'), (2, u'0.024*\"could\" + 0.017*\"temple\" + 0.017*\"never\"')]\n",
      "chap6 [(0, u'0.057*\"shock\" + 0.030*\"intercom\" + 0.030*\"teacher\"'), (1, u'0.034*\"shock\" + 0.018*\"error\" + 0.018*\"punishment\"'), (2, u'0.044*\"teacher\" + 0.028*\"answer\" + 0.028*\"test\"')]\n",
      "chap5 [(0, u'0.030*\"friend\" + 0.023*\"tupperware\" + 0.023*\"like\"'), (1, u'0.026*\"party\" + 0.020*\"home\" + 0.020*\"sale\"'), (2, u'0.031*\"seem\" + 0.018*\"woman\" + 0.018*\"friend\"')]\n",
      "chap4 [(0, u'0.025*\"audience\" + 0.017*\"humorous\" + 0.017*\"know\"'), (1, u'0.027*\"canned\" + 0.027*\"laugh\" + 0.027*\"material\"'), (2, u'0.025*\"correct\" + 0.017*\"principle\" + 0.017*\"social\"')]\n",
      "chap3 [(0, u'0.029*\"tim\" + 0.016*\"time\" + 0.016*\"together\"'), (1, u'0.053*\"tim\" + 0.038*\"sara\" + 0.017*\"drinking\"'), (2, u'0.033*\"sara\" + 0.033*\"tim\" + 0.025*\"boyfriend\"')]\n",
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.029*blurred + 0.025*money + 0.020*nothing'), (1, u'0.021*phone + 0.020*similar + 0.020*decision'), (2, u'0.037*range + 0.028*fact + 0.026*stance')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap6\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=220)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.030*shocks\\xf0in + 0.027*test + 0.025*quickly'), (1, u'0.036*frenzied + 0.027*enough + 0.027*disruptive'), (2, u'0.027*electrode + 0.026*confused + 0.023*quickly')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap5\n",
      "18\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 18\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=18, num_nnz=210)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.031*pushed + 0.031*effective + 0.029*purchase'), (1, u'0.028*friend + 0.028*recruit + 0.027*everyone'), (2, u'0.035*consumer + 0.033*preference + 0.029*product')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap4\n",
      "22\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 22\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=22, num_nnz=223)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.036*widespread + 0.030*effective + 0.024*ming'), (1, u'0.043*research + 0.028*fabricated + 0.028*popcorn'), (2, u'0.041*thing + 0.029*state + 0.026*well')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap3\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=236)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.032*marriage + 0.031*sara + 0.026*really'), (1, u'0.027*yet + 0.024*quit + 0.022*change'), (2, u'0.036*offered + 0.032*called + 0.023*enough')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap2\n",
      "15\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 15\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=15, num_nnz=250)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.040*surprising + 0.026*help + 0.020*victim'), (1, u'0.025*surprised + 0.021*sense + 0.021*give'), (2, u'0.037*world + 0.025*sophisticated + 0.024*gouldner')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "6\n",
      "chap7 [(0, u'0.021*\"breast\" + 0.021*\"health\" + 0.015*\"potential\"'), (1, u'0.020*\"could\" + 0.020*\"much\" + 0.020*\"money\"'), (2, u'0.023*\"something\" + 0.016*\"become\" + 0.016*\"thought\"')]\n",
      "chap6 [(0, u'0.046*\"shock\" + 0.041*\"teacher\" + 0.024*\"test\"'), (1, u'0.032*\"shock\" + 0.023*\"take\" + 0.023*\"get\"'), (2, u'0.031*\"answer\" + 0.031*\"test\" + 0.022*\"next\"')]\n",
      "chap5 [(0, u'0.030*\"friend\" + 0.024*\"party\" + 0.024*\"tupperware\"'), (1, u'0.029*\"buy\" + 0.021*\"home\" + 0.020*\"take\"'), (2, u'0.026*\"product\" + 0.026*\"sale\" + 0.026*\"social\"')]\n",
      "chap4 [(0, u'0.026*\"social\" + 0.026*\"principle\" + 0.018*\"correct\"'), (1, u'0.018*\"action\" + 0.018*\"behavior\" + 0.018*\"well\"'), (2, u'0.033*\"canned\" + 0.033*\"laughter\" + 0.026*\"laugh\"')]\n",
      "chap3 [(0, u'0.058*\"tim\" + 0.021*\"even\" + 0.020*\"sara\"'), (1, u'0.043*\"sara\" + 0.037*\"tim\" + 0.014*\"still\"'), (2, u'0.027*\"time\" + 0.015*\"done\" + 0.015*\"already\"')]\n",
      "chap2 [(0, u'0.018*\"mexico\" + 0.018*\"ethiopia\" + 0.018*\"thousand\"'), (1, u'0.016*\"one\" + 0.015*\"person\" + 0.015*\"society\"'), (2, u'0.023*\"human\" + 0.017*\"system\" + 0.012*\"exchange\"')]\n",
      "chap7\n",
      "14\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 14\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=14, num_nnz=211)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.028*people + 0.025*ring + 0.020*becoming'), (1, u'0.036*expect + 0.025*mind + 0.023*begun'), (2, u'0.041*question + 0.022*breast + 0.020*principle')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap6\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=220)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.044*chair + 0.031*tolerable + 0.031*way'), (1, u'0.049*wait + 0.029*respond + 0.029*starting'), (2, u'0.037*permanent + 0.027*incorrect + 0.024*possibly')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap5\n",
      "18\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 18\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=18, num_nnz=210)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.025*yes + 0.023*result + 0.022*refreshment'), (1, u'0.025*call + 0.024*know + 0.020*interesting'), (2, u'0.026*retail + 0.023*appear + 0.021*avoid')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap4\n",
      "22\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=22, num_nnz=223)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.034*afloat + 0.027*way + 0.021*thing'), (1, u'0.034*theater + 0.033*one + 0.033*others'), (2, u'0.035*effective + 0.033*mechanically + 0.028*introduction')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap3\n",
      "24\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 24\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=24, num_nnz=236)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.027*people + 0.027*weshould + 0.025*willingness'), (1, u'0.030*volunteered + 0.027*done + 0.025*would'), (2, u'0.025*difficult + 0.024*say + 0.020*dated')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap2\n",
      "15\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 15\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=15, num_nnz=250)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.019*mexico + 0.018*person + 0.017*result'), (1, u'0.025*indebtedness + 0.022*flowing + 0.021*war'), (2, u'0.032*\\xaaweb + 0.025*shared + 0.025*newspaper')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "chap1\n",
      "10\n",
      "Entering to Topic Modeling\n",
      "Length of doc_clean list 10\n",
      "Topics unigram\n",
      "TFIDF model\n",
      "TfidfModel(num_docs=10, num_nnz=116)\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[(0, u'0.051*\\xaapush\\xba + 0.044*recentlyopened + 0.042*piece'), (1, u'0.054*friend + 0.038*staff + 0.036*trick'), (2, u'0.061*tourist + 0.042*case + 0.039*shocked')]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "**************************************************\n",
      "7\n",
      "chap7 [(0, u'0.020*\"temple\" + 0.020*\"never\" + 0.014*\"something\"'), (1, u'0.022*\"could\" + 0.021*\"money\" + 0.021*\"told\"'), (2, u'0.023*\"breast\" + 0.023*\"health\" + 0.016*\"potential\"')]\n",
      "chap6 [(0, u'0.037*\"answer\" + 0.037*\"test\" + 0.028*\"teacher\"'), (1, u'0.048*\"shock\" + 0.032*\"teacher\" + 0.018*\"make\"'), (2, u'0.038*\"shock\" + 0.031*\"teacher\" + 0.031*\"intercom\"')]\n",
      "chap5 [(0, u'0.029*\"friend\" + 0.015*\"every\" + 0.015*\"come\"'), (1, u'0.018*\"like\" + 0.018*\"product\" + 0.018*\"buy\"'), (2, u'0.023*\"home\" + 0.023*\"party\" + 0.023*\"tupperware\"')]\n",
      "chap4 [(0, u'0.020*\"social\" + 0.020*\"principle\" + 0.020*\"executive\"'), (1, u'0.031*\"correct\" + 0.022*\"action\" + 0.022*\"others\"'), (2, u'0.037*\"material\" + 0.037*\"laugh\" + 0.028*\"canned\"')]\n",
      "chap3 [(0, u'0.030*\"tim\" + 0.012*\"plan\" + 0.012*\"called\"'), (1, u'0.042*\"sara\" + 0.035*\"tim\" + 0.015*\"boyfriend\"'), (2, u'0.046*\"tim\" + 0.033*\"sara\" + 0.021*\"together\"')]\n",
      "chap2 [(0, u'0.018*\"society\" + 0.018*\"human\" + 0.010*\"rule\"'), (1, u'0.018*\"human\" + 0.014*\"obligation\" + 0.014*\"future\"'), (2, u'0.022*\"away\" + 0.013*\"year\" + 0.013*\"resource\"')]\n",
      "chap1 [(0, u'0.032*\"piece\" + 0.032*\"even\" + 0.018*\"sale\"'), (1, u'0.027*\"sold\" + 0.027*\"store\" + 0.027*\"jewelry\"'), (2, u'0.023*\"friend\" + 0.023*\"day\" + 0.023*\"luck\"')]\n"
     ]
    }
   ],
   "source": [
    "chaps = {}\n",
    "\n",
    "book = read_book(book_name)\n",
    "for k,v in topics.items():\n",
    "\n",
    "    chaps[k] = get_text(book, v['start'], v['end'])\n",
    "\n",
    "    chapterwise_topics = {}\n",
    "\n",
    "    chapterwise_bigrm_topics = {}\n",
    "\n",
    "    for k,v in chaps.items():\n",
    "\n",
    "        print k\n",
    "\n",
    "        text = v[0]\n",
    "\n",
    "        sentences = get_sentences(text)\n",
    "\n",
    "        print len(sentences)\n",
    "\n",
    "        end = len(sentences)\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        doc_clean = []\n",
    "\n",
    "        bigrm_doc = []\n",
    "\n",
    "        for idx in range(start,end,1):\n",
    "\n",
    "            sent = sentences[idx]\n",
    "\n",
    "\n",
    "            words = get_words_list(sent)\n",
    "\n",
    "            stop_free_words = remove_stopwords(words)\n",
    "\n",
    "\n",
    "            punct_free_words = remove_punctuations_and_small_words(stop_free_words, 2)\n",
    "\n",
    "\n",
    "            stemmed = stem_words(punct_free_words)\n",
    "\n",
    "\n",
    "            lemmatized = lemmatize_words(punct_free_words)\n",
    "\n",
    "\n",
    "            #Bigrams\n",
    "\n",
    "            bigrm = list(nltk.bigrams(lemmatized))\n",
    "\n",
    "\n",
    "            bigrm_doc.append([' '.join((a,b)) for a,b in bigrm])\n",
    "\n",
    "            \n",
    "\n",
    "            pos_text = pos_tagging(lemmatized)\n",
    "\n",
    "            \n",
    "\n",
    "            tag_fd = [tag for (word, tag) in pos_text]\n",
    "\n",
    "            \n",
    "\n",
    "            noun_text = get_pos_text(pos_text, [\"NN\", \"VBG\", \"VBN\"])\n",
    "\n",
    "            \n",
    "\n",
    "            doc_clean.append(lemmatized)\n",
    "\n",
    "\n",
    "        print \"Entering to Topic Modeling\"\n",
    "\n",
    "        print \"Length of doc_clean list %s\" %len(doc_clean)\n",
    "\n",
    "        #print doc_clean\n",
    "\n",
    "        dictionary = get_dict(doc_clean)\n",
    "\n",
    "        doc_term_matrix = get_df_matrix(doc_clean, dictionary)\n",
    "\n",
    "        \n",
    "\n",
    "        topic_count = 3\n",
    "\n",
    "        ldamodel = fit_lda_model(doc_term_matrix, dictionary, topic_count)\n",
    "\n",
    "        print(\"Topics unigram\")\n",
    "\n",
    "        \n",
    "\n",
    "        chapterwise_topics[k] = ldamodel\n",
    "\n",
    "        print \"TFIDF model\"\n",
    "\n",
    "        tfidfmodel = fit_tfidf_model(doc_term_matrix)\n",
    "\n",
    "        print tfidfmodel\n",
    "\n",
    "        \n",
    "\n",
    "        print 50*\"@\"\n",
    "\n",
    "        hdp = fit_hdp_model(doc_term_matrix, dictionary)\n",
    "\n",
    "        print hdp.print_topics(num_topics=3, num_words=3)\n",
    "\n",
    "        print 50*\"@\"\n",
    "\n",
    "\n",
    "    print 50 * \"*\"\n",
    "\n",
    "    print len(chapterwise_topics)\n",
    "\n",
    "    for chap, model in chapterwise_topics.items():\n",
    "\n",
    "        print chap, model.print_topics(num_topics=3, num_words=3)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        from wordcloud import WordCloud\n",
    "\n",
    "        for t in range(model.num_topics):\n",
    "\n",
    "            wc = WordCloud().fit_words(dict(model.show_topic(t, 200)))\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            plt.imshow(wc, interpolation='bilinear')\n",
    "\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.title(\"Topic #\" + k+str(t))\n",
    "\n",
    "            \n",
    "\n",
    "            plt.savefig(chap+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
